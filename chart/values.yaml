## Added to upstream chart to support istio
hostname: dev.bigbang

istio:
  # Toggle istio interaction
  enabled: false
  fluentd:
    enabled: true
    annotations: {}
    labels: {}
    gateways:
      - istio-system/main
    hosts:
      - fluentd.{{ .Values.hostname }}
## End of addition 

nameOverride: ""
fullnameOverride: ""

# DaemonSet or Deployment
kind: "Deployment" # Changed from upstream

# # Only applicable for Deployment
replicaCount: 1 # Changed from upstream

image:
  repository: registry1.dso.mil/ironbank/bigbang/fluentd-aggregator
  pullPolicy: "Always"
  tag: "1.14.2"

## Optional array of imagePullSecrets containing private registry credentials
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets:
- name: private-registry

serviceAccount:
  create: true
  annotations: {}
  name: null

rbac:
  create: true

# Configure podsecuritypolicy
# Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
podSecurityPolicy:
  enabled: true
  annotations: {}

## Security Context policies for controller pods
## See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for
## notes on enabling and using sysctls
##
podSecurityContext: {}
  # seLinuxOptions:
  #   type: "spc_t"

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# Configure the livessProbe
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /metrics
    port: metrics
  # initialDelaySeconds: 0
  # periodSeconds: 10
  # timeoutSeconds: 1
  # successThreshold: 1
  # failureThreshold: 3

# Configure the readinessProbe
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
readinessProbe:
  httpGet:
    path: /metrics
    port: metrics
  # initialDelaySeconds: 0
  # periodSeconds: 10
  # timeoutSeconds: 1
  # successThreshold: 1
  # failureThreshold: 3

resources: {}
  # requests:
  #   cpu: 10m
  #   memory: 128Mi
  # limits:
  #   memory: 128Mi

## only available if kind is Deployment
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
  ## see https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
  customRules: []
    # - type: Pods
    #   pods:
    #     metric:
    #       name: packets-per-second
    #     target:
    #       type: AverageValue
    #       averageValue: 1k
  ## see https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-configurable-scaling-behavior
  # behavior:
  #   scaleDown:
  #     policies:
  #       - type: Pods
  #         value: 4
  #         periodSeconds: 60
  #       - type: Percent
  #         value: 10
  #         periodSeconds: 60

# priorityClassName: "system-node-critical"

nodeSelector: {}

## Node tolerations for server scheduling to nodes with taints
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
##
tolerations: []
# - key: null
#   operator: Exists
#   effect: "NoSchedule"

## Affinity and anti-affinity
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity: {}

## Annotations to be added to fluentd DaemonSet/Deployment
##
annotations: {}

## Labels to be added to fluentd DaemonSet/Deployment
##
labels: {}

## Annotations to be added to fluentd pods
##
podAnnotations: {}

## Labels to be added to fluentd pods
##
podLabels: {}

## Deployment strategy / DaemonSet updateStrategy
##
updateStrategy: {}
#   type: RollingUpdate
#   rollingUpdate:
#     maxUnavailable: 1

## Additional environment variables to set for fluentd pods
env:
  - name: "FLUENTD_CONF" # Added for IB image
    value: "fluent.conf" # Added for IB image 
  - name: "FLUENT_ELASTICSEARCH_DATA_STREAM_NAME" # Added to chart
    value: "logs-k8s-cluster" # Added to chart
  - name: "FLUENT_FORWARD_PASSWORD" # Added to chart
    value: "secret" # Added to chart
# - name: FLUENTD_CONF"  # Commented out from upstream chart
#   value: "../../etc/fluent/fluent.conf"
  # - name: FLUENT_ELASTICSEARCH_HOST
  #   value: "elasticsearch-master"
  # - name: FLUENT_ELASTICSEARCH_PORT
  #   value: "9200"

envFrom: []

volumes:
# - name: varlog # Commented out  as the default BB deployment is not used to collect logs
#   hostPath:
#     path: /var/log
# - name: varlibdockercontainers
#   hostPath:
#     path: /var/lib/docker/containers
- name: etcfluentd-main
  configMap:
    name: fluentd-main
    defaultMode: 0777
- name: etcfluentd-config
  configMap:
    name: fluentd-config
    defaultMode: 0777

volumeMounts:
## Changed from upstream (commented out the two volume mounts)
# - name: varlog
#   mountPath: /var/log
# - name: varlibdockercontainers
#   mountPath: /var/lib/docker/containers
#   readOnly: true
- name: etcfluentd-main
  mountPath: /fluentd/etc/ # changed for IB image
  # mountPath: /etc/fluent 
- name: etcfluentd-config
  mountPath: /fluentd/etc/config.d/ # changed for IB image
  # mountPath: /etc/fluent/config.d/ 

## Fluentd service
service:
  type: "ClusterIP"
  annotations: {}
  ports:
  ## changed from upstream (uncommenting the ports below) 
  - name: "forwarder"
    protocol: TCP
    containerPort: 24224 ## this correlates to the <source> you define in your fluentd config
    tcpPort: 24225

## Prometheus Monitoring
##
metrics:
  serviceMonitor:
    enabled: false
    additionalLabels:
      release: prometheus-operator
    namespace: ""
    namespaceSelector: {}
    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: ^fluentd_output_status_buffer_(oldest|newest)_.+
    #   replacement: $1
    #   action: drop
    ## relabel configs to apply to samples after ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional serviceMonitor config
    ##
    # jobLabel: fluentd
    # scrapeInterval: 30s
    # scrapeTimeout: 5s
    # honorLabels: true

  prometheusRule:
    enabled: false
    additionalLabels: {}
    namespace: ""
    rules: []
    # - alert: FluentdDown
    #   expr: up{job="fluentd"} == 0
    #   for: 5m
    #   labels:
    #     context: fluentd
    #     severity: warning
    #   annotations:
    #     summary: "Fluentd Down"
    #     description: "{{ $labels.pod }} on {{ $labels.nodename }} is down"
    # - alert: FluentdScrapeMissing
    #   expr: absent(up{job="fluentd"} == 1)
    #   for: 15m
    #   labels:
    #     context: fluentd
    #     severity: warning
    #   annotations:
    #     summary: "Fluentd Scrape Missing"
    #     description: "Fluentd instance has disappeared from Prometheus target discovery"

## Grafana Monitoring Dashboard
##
dashboards:
  enabled: "true"
  namespace: ""
  labels:
    grafana_dashboard: '"1"'

## Changed from upstream (IB image doesn't support dynamic plugin installation at runtime)
# ## Fluentd list of plugins to install
# ##
# plugins: []
# # - fluent-plugin-out-http

## Add fluentd config files from K8s configMaps
##
configMapConfigs:
  - fluentd-prometheus-conf
# - fluentd-systemd-conf

## Added to upstream chart to support elasticsearch integration
## Elasticsearch info
elasticsearch:
  host: logging-ek-es-http.logging.svc.cluster.local
  port: 9200
  user: elastic
  ## set password directly using elastic_password, or set the secret and key to use to grab the password.
  elastic_password: ""
  password:
    ## This is the secret to lookup and extract the password from
    ## currently the secret is expected to have the value `.data.elastic`
    ## which is the default format for elasticsearch 
    secret: logging-ek-es-elastic-user
    namespace: logging
## End of addition 

## Fluentd configurations:
##
fileConfigs:
  01_sources.conf: |-
    ## Changed from upstream helm chart
    ## Ignore fluentd own events
    <match fluent.**>
      @type null
    </match>
    <source>
      @type forward
      @log_level info
      port 24224
      bind 0.0.0.0
      <security>
        self_hostname {{ tpl (index .Values.istio.fluentd.hosts 0) $ }}
        shared_key "#{ENV["FLUENT_FORWARD_PASSWORD"]}"
      </security>
    </source>
    # ## logs from podman
    # <source>
    #   @type tail
    #   @id in_tail_container_logs
    #   @label @KUBERNETES
    #   path /var/log/containers/*.log
    #   pos_file /var/log/fluentd-containers.log.pos
    #   tag kubernetes.*
    #   read_from_head true
    #   <parse>
    #     @type multi_format
    #     <pattern>
    #       format json
    #       time_key time
    #       time_type string
    #       time_format "%Y-%m-%dT%H:%M:%S.%NZ"
    #       keep_time_key false
    #     </pattern>
    #     <pattern>
    #       format regexp
    #       expression /^(?<time>.+) (?<stream>stdout|stderr)( (.))? (?<log>.*)$/
    #       time_format '%Y-%m-%dT%H:%M:%S.%NZ'
    #       keep_time_key false
    #     </pattern>
    #   </parse>
    #   emit_unmatched_lines true
    # </source>

  02_filters.conf: |-
    ## Adding to upstream chart
    <match **>
      @type relabel
      @label @DISPATCH
    </match>
    # <filter **>
    # @type record_modifier
    #   <record>
    #     some_key "some_value"
    #   </record>
    # </filter>
    ## End Addition

    ## Commenting out from upstream chart
    # <label @KUBERNETES>
    #   <match kubernetes.var.log.containers.fluentd**>
    #     @type relabel
    #     @label @FLUENT_LOG
    #   </match>

    #   # <match kubernetes.var.log.containers.**_kube-system_**>
    #   #   @type null
    #   #   @id ignore_kube_system_logs
    #   # </match>

    #   <filter kubernetes.**>
    #     @type kubernetes_metadata
    #     @id filter_kube_metadata
    #     skip_labels false
    #     skip_container_metadata false
    #     skip_namespace_metadata true
    #     skip_master_url true
    #   </filter>

    #   <match **>
    #     @type relabel
    #     @label @DISPATCH
    #   </match>
    # </label>
  03_dispatch.conf: |-
    <label @DISPATCH>
      <filter **>
        @type prometheus
        <metric>
          name fluentd_input_status_num_records_total
          type counter
          desc The total number of incoming records
          <labels>
            tag ${tag}
            hostname ${hostname}
          </labels>
        </metric>
      </filter>

      <match **>
        @type relabel
        @label @OUTPUT
      </match>
    </label>

  04_outputs.conf: |-
    ## Adding to upstream chart
    <label @OUTPUT>
      <match **>
        @type elasticsearch_data_stream
        # ECS Naming convention for data_streams
        # https://www.elastic.co/guide/en/elasticsearch/reference/master/set-up-a-data-stream.html#elastic-data-stream-naming-scheme
        # <type>-<dataset>-<namespace>
        data_stream_name "#{ENV["FLUENT_ELASTICSEARCH_DATA_STREAM_NAME"]}"
        host  "#{ENV["FLUENT_ELASTICSEARCH_HOST"]}"
        port "#{ENV["FLUENT_ELASTICSEARCH_PORT"]}"
        user "#{ENV["FLUENT_ELASTICSEARCH_USER"]}"
        password "#{ENV["FLUENT_ELASTICSEARCH_PASSWORD"]}"
        logstash_format false
        scheme https
        ssl_verify false
        reload_connections false
        request_timeout 60s
        ssl_version TLSv1_2
        bulk_message_request_threshold 5M
        <buffer>
          flush_interval 10s
        </buffer>
      </match>
    </label>
    ## End addition
    ## Commenting out from upstream
    # <label @OUTPUT>
    #   <match **>
    #     @type elasticsearch
    #     host "elasticsearch-master"
    #     port 9200
    #     path ""
    #     user elastic
    #     password changeme
    #   </match>
    # </label>
